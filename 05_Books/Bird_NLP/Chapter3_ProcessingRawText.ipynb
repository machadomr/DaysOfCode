{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "resident-distributor",
   "metadata": {},
   "source": [
    "# <center>Book: Steven Bird, Ewan Klein, Edward Loper, 2009. **Natural Language Processing (NLP) with Python**, O'Reilly.</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-theology",
   "metadata": {},
   "source": [
    "This notebook is an exploration of the solutions proposed by the user:\n",
    "    https : // github.com / Sturz gef ahr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-product",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\">Chapter #3</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extended-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first import all we will need for these first questions\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import state_union\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-principal",
   "metadata": {},
   "source": [
    "###### 1. \n",
    "\n",
    "☼ Define a string `s = 'colorless'`. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brazilian-terminal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s = s[:4] + 'u' + s[4:]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-diamond",
   "metadata": {},
   "source": [
    "##### 2.\n",
    "\n",
    "☼ We can use the slice notation to remove morphological endings on words. For example, `'dogs'[:-1]` removes the last character of `dogs`, leaving `dog`. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): `dish-es`, `run-ning`, `nation-ality`, `un-do`, `pre-heat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mathematical-sacramento",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish', 'run', 'nation', 'un', 'pre']\n"
     ]
    }
   ],
   "source": [
    "affixed = [('dishes', 2), \n",
    "           ('running', 4),\n",
    "           ('nationality', 5),\n",
    "           ('undo', 2),\n",
    "           ('preheat', 4)]\n",
    "\n",
    "print([s[:-a] for s, a in affixed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-wrist",
   "metadata": {},
   "source": [
    "##### 4. \n",
    "\n",
    "☼ We can specify a \"step\" size for the slice. The following returns every second character within the slice: `monty[6:11:2]`. It also works in the reverse direction: `monty[10:5:-2]` Try these for yourself, then experiment with different step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "welsh-latest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mnalnu aen  otge'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Portuguese:\n",
    "tt = 'Minha lingua materna e portugues'\n",
    "# Every other letter\n",
    "tt[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecological-extent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suurpeartmagi hi'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every other letter from the end\n",
    "tt[::-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "settled-chassis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mhlg tneoue'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every third letter\n",
    "tt[::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-praise",
   "metadata": {},
   "source": [
    "*You get the point...*\n",
    "\n",
    "##### 5. \n",
    "\n",
    "☼ What happens if you ask the interpreter to evaluate `monty[::-1]`? Explain why this is a reasonable result.\n",
    "\n",
    "*It prints the word backwards.  It's simply printing from the end by steps of -1:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "contained-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'murder'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"redrum\"[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-thickness",
   "metadata": {},
   "source": [
    "##### 6.\n",
    "\n",
    "☼ Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "a. `[a-zA-Z]+`\n",
    "\n",
    "b. `[A-Z][a-z]*`\n",
    "\n",
    "c. `p[aeiou]{,2}t`\n",
    "\n",
    "d. `\\d+(\\.\\d+)?`\n",
    "\n",
    "e. `([^aeiou][aeiou][^aeiou])*`\n",
    "\n",
    "f. `\\w+|[^\\w\\s]+`\n",
    "\n",
    "Test your answers using `nltk.re_show()`.\n",
    "\n",
    "*__a.__ `[a-zA-Z]+` will match anything alphabetical:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sufficient-organization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{cAMELCASE} 6186258313 {hybr}1{d}\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "\n",
    "nltk.re_show(r'[a-zA-Z]+', \"cAMELCASE 6186258313 hybr1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lesser-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{This} is just another random sentence, outra {Ran}{D}{O}{M} sentence.\n"
     ]
    }
   ],
   "source": [
    "test = 'This is just another random sentence, ' \\\n",
    "       'outra RanDOM sentence.'\n",
    "\n",
    "nltk.re_show(r'[A-Z][a-z]*', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "environmental-captain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6978"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = [w.lower() for w in nltk.corpus.words.words('en')]\n",
    "len([w for w in wordlist if re.search(r'p[aeiou]{,2}t', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "israeli-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abaptiston', 'abepithymia', 'ableptical', 'ableptically', 'abrupt', 'abruptedly', 'abruption', 'abruptly', 'abruptness', 'absorpt', 'absorptance', 'absorptiometer', 'absorptiometric', 'absorption', 'absorptive', 'absorptively', 'absorptiveness', 'absorptivity', 'absumption', 'acalypterae']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search(r'p[aeiou]{,2}t', w)][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "equivalent-catalog",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pat', 'pat', 'paut', 'peat', 'pet', 'piet', 'piet', 'pit', 'poet', 'poot', 'pot', 'pout', 'put']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search(r'^p[aeiou]{,2}t$', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "processed-medium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1234}\n",
      "{12.34}\n",
      "example {123.4} in a string\n",
      "{1}-{234}\n",
      "{12},{4}\n",
      "${12.34}\n"
     ]
    }
   ],
   "source": [
    "test = ['1234', '12.34', 'example 123.4 in a string', '1-234', '12,4', '$12.34']\n",
    "for t in test:\n",
    "    nltk.re_show(r'\\d+(\\.\\d+)?', t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prescribed-telling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.23}.{4}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(\\.\\d+)?', '1.23.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "otherwise-allergy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.23.4}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(\\.\\d+)+', '1.23.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-sterling",
   "metadata": {},
   "source": [
    "<i>__e.__ `([^aeiou][aeiou][^aeiou])*` will match any non-vowel\\vowel\\non-vowel combination, no matter how many times it's repeated.  White spaces are considered non-vowels, so a string such as `to ` would match.  `nltk.re_show()` behaves quite strangely with this RegExp - a string like `\"baab\"` would return `{}b{}a{}a{}b{}`.  However, I have evaluated this RegExp with online evaluators (such as [this one](https://regexr.com/ \"regexr.com\"), and there the responsive is as expected:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "chubby-destination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{babbabbabbab}{}a{pap}{}a{}\n"
     ]
    }
   ],
   "source": [
    "string = \"babbabbab\" \\\n",
    "         \"babapapa\"\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "amazing-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}b{}a{}a{}b{}\n"
     ]
    }
   ],
   "source": [
    "string = \"baab\"\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-integral",
   "metadata": {},
   "source": [
    "*__f.__ `\\w+|[^\\w\\s]+` will match either any alphanumeric string of any length, or a string of any length that does not contain alphanumeric characters or whitespace - i.e., all punctuation and any other non-whitespace/non-alphanumeric characters:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "polyphonic-petite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{This} {RegExp} {needs} {a} {fairly} {long} {string} {to} {show} {what} {it} {can} {%#$^%&*} {do}{.}\n"
     ]
    }
   ],
   "source": [
    "string = \"This RegExp needs a fairly long string to show what it can %#$^%&* do.\"\n",
    "nltk.re_show(r'\\w+|[^\\w\\s]+', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-night",
   "metadata": {},
   "source": [
    "##### 7.\n",
    "\n",
    "*☼ Write regular expressions to match the following classes of strings:*\n",
    "\n",
    " + *__a.__ A single determiner (assume that __a__, __an__, and __the__ are the only determiners).*\n",
    " + <i>__b.__ An arithmetic expression using integers, addition, and multiplication, such as `2*3+8`.</i>\n",
    " \n",
    "*__a.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "enclosed-importance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence is just {an} example to be tested.\n"
     ]
    }
   ],
   "source": [
    "string = \"This sentence is just an example to be tested.\"\n",
    "nltk.re_show(r'\\b[Aa]n?\\b|\\b[Tt]he\\b', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-voltage",
   "metadata": {},
   "source": [
    "*__b.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "anticipated-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2 * 3 + 8}\n"
     ]
    }
   ],
   "source": [
    "string = \"2 * 3 + 8\"\n",
    "nltk.re_show(r'(\\d|[+*= ])+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "indirect-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{11 + 4 * 2}\n"
     ]
    }
   ],
   "source": [
    "string = \"11 + 4 * 2\"\n",
    "nltk.re_show(r'(\\d|[+*= ])+', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-intervention",
   "metadata": {},
   "source": [
    "##### 8.\n",
    "\n",
    "\n",
    "☼ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use `from urllib import request`  and then `request.urlopen('http://nltk.org/').read().decode('utf8')` to access the contents of the URL.\n",
    "\n",
    "*The code below is inspired in [this answer in the above Stack Overflow discussion](https://stackoverflow.com/a/30565597 \"Removing Style, Scripts, and HTML tags - answer\"):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "outside-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize\n",
    "\n",
    "def return_URL_contents(url):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(html, 'html.parser')\n",
    "    for r in raw(['script', 'style']):\n",
    "        r.extract() # remove tags\n",
    "    \n",
    "    text = ' '.join(raw.stripped_strings) # retrieve tag content\n",
    "    \n",
    "    return normalize('NFKD', text) # normalize escape sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minus-operation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What Virtual Reality Can Teach a Driverless Car - The New York Times Sections SEARCH Skip to content Skip to site index Business Log in Today’s Paper Business | What Virtual Reality Can Teach a Driverless Car Artificial Intelligence The Bot That Writes Are These People Real? Algorithms Against Suicide Robots Without Bias Advertisement Continue reading the main story Supported by Continue reading the main story What Virtual Reality Can Teach a Driverless Car By Cade Metz Oct. 29, 2017 SAN FRANCISCO — As the computers that operate driverless cars digest the rules of the road, some engineers think it might be nice if they can learn from mistakes made in virtual reality rather than on real streets. Companies like Toyota, Uber and Waymo have discussed at length how they are testing autonomous vehicles on the streets of Mountain View, Calif., Phoenix and other cities. What is not as well known is that they are also testing vehicles inside computer simulations of these same cities. Virtual cars, equipped with the same software as the real thing, spend thousands of hours driving their digital worlds. Think of it as a way of identifying flaws in the way the cars operate without endangering real people. If a car makes a mistake on a simulated drive, engineers can tweak its software accordingly, laying down new rules of behavior. On Monday, Waymo, the autonomous car company that spun out of Google, is expected to show off its simulator tests when it takes a group of reporters to its secretive testing center in California’s Central Valley. Researchers are also developing methods that would allow cars to actually learn new behavior from these simulations, gathering skills more quickly than human engineers could ever lay them down with explicit software code. “Simulation is a tremendous thing,” said Gill Pratt, chief executive of the Toyota Research Institute, one of the artificial intelligence labs exploring this kind of virtual training for autonomous vehicles and other robotic'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.nytimes.com/2017/10/29/business/virtual-reality-driverless-cars.html?module=inline\"\n",
    "\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "banned-worthy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guido van Rossum - Wikipedia Guido van Rossum From Wikipedia, the free encyclopedia Jump to navigation Jump to search Dutch programmer and creator of Python \"GvR\" redirects here. For other uses, see gvr (disambiguation) . In this Dutch name , the surname is van Rossum . Guido van Rossum Van Rossum at the Dropbox headquarters in 2014 Born ( 1956-01-31 ) 31 January 1956 (age 65) [1] Haarlem , Netherlands [2] [3] Nationality Dutch Alma mater University of Amsterdam Occupation Computer programmer, author Known for Creating the Python programming language Spouse(s) Kim Knapp \\u200b ( m. 2000) \\u200b Children 1 [4] Awards Award for the Advancement of Free Software (2001) Website gvanrossum .github .io Guido van Rossum ( Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm] ; born 31 January 1956) is a Dutch programmer best known as the creator of the Python programming language , for which he was the \" Benevolent dictator for life \" (BDFL) until he stepped down from the position in July 2018. [5] [6] He remained a member of the Python Steering Council through 2019, and withdrew from nominations for the 2020 election. [7] Contents 1 Life and education 2 Work 2.1 Python 2.2 1999 \"Computer Programming for Everybody\" proposal 2.3 Google 2.4 Dropbox 2.5 Microsoft 3 Awards 4 References 5 External links Life and education [ edit ] Van Rossum was born and raised in the Netherlands , where he received a master\\'s degree in mathematics and computer science from the University of Amsterdam in 1982. He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the \"Python Powered\" logo. [8] Van Rossum lives in Belmont , California, with his wife, Kim Knapp, [9] and their son. [10] [11] [12] According to his home page and Dutch naming conventions , the \" van \" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together. [13] Work [ edit ] While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Guido_van_Rossum\"\n",
    "\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-sacrifice",
   "metadata": {},
   "source": [
    "##### 9. \n",
    "\n",
    "☼ Save some text into a file `corpus.txt`. Define a function `load(f)` that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    " + a. Use `nltk.regexp_tokenize()` to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag `(?x)`.\n",
    " \n",
    " + b. Use `nltk.regexp_tokenize()` to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sporting-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cbc.ca/news/world/coronavirus-covid19-canada-world-february21-2021-1.5922099'\n",
    "\n",
    "text = return_URL_contents(url)\n",
    "\n",
    "with open('corpus.txt', 'w', encoding = \"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "modified-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    text = open(f, encoding = \"utf-8\")\n",
    "    raw = text.read()\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "labeled-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt = load('corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-taiwan",
   "metadata": {},
   "source": [
    "*__a.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "experimental-settlement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', \"'\", ':', \"'\", ',', '.', '.', '.', ':', ',', ':', ':', \"'\", '.', '.', '.', '.', '(', ')', ':', '.', '.', ',', '.', '.', \"'\", ',', '.', '.', '.', ':', ',', '.', '?', '.', ',', ',', '\"', '\"', '.', '.', \"'\", '.', ',', '.', ',', ',', '.', '.', '.', '\"', '\"', '.', \"'\", ',', ',', '.', '.', ',', \"'\", ',', '.', '.', ',', ',', '.', ',', '.', '.', \"'\", '.', '\"', '\"', '.', \"'\", '?', ',', \"'\", ',', '.', ',', ',', '.', ',', '\"', ',', ',', ',', '.', '\"', ',', '.', '.', \"'\", '.', '.', '(', ')', '.', ',', '.', '.', '.', '.', '.', ',', '.', ',', ',', ',', ',', '.', ',', ',', '.', '.', \"'\", ':', \"'\", ':', ',', \"'\", '.', \"'\", ',', '.', '.', ':', '.', ',', '.', '.', \"'\", ':', '.', '.', ',', ',', ',', ',', '.', ',', '.', ',', '.', '.', ',', ',', \"'\", '.', '.', '.', '.', \"'\", \"'\", ',', '.', '.', '.', ':', ':', ',', '.', '.', ':', '.', ',', ',', '.', '.', ',', ',', '.', ',', '.', ',', '.', '.', \"'\", '.', ',', '.', ',', \"'\", '.', \"'\", ',', '.', ',', '.', '.', '.', '.', '.', '.', '.', \"'\", '.', ',', '.', '.', '.', '.', ':', '.', '.', ',', '.', '.', ',', ',', '.', ',', ',', \"'\", '.', ',', \"'\", ',', '.', ',', ',', '.', '(', ')', ',', ',', ',', ',', '.', ',', '.', ',', ',', '.', ',', ',', ',', '.', '.', '.', '(', ')', ',', ',', ',', ',', ',', '.', \"'\", '.', '.', '.', \"'\", '.', ',', \"'\", '.', \"'\", ',', '.', ',', \"'\", '(', ')', '.', '.', ',', ',', '.', '.', '.', '.', '.', ',', '.', '.', ',', ',', '(', ')', ':', ':', ',', ',', '.', '.', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "    [][.,;\"'?!():_-`] # finds punctuation\n",
    "'''\n",
    "\n",
    "print(nltk.regexp_tokenize(nyt, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-playing",
   "metadata": {},
   "source": [
    "*__b.__ Using regular expressions to extract information such as proper names - which can take numerous forms - is wrought with problems, and the regular expressions below are far from perfect.  It could very well be that the point of this exercise was to demonstrate just how difficult this approach is.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "straight-perth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['News Skip', 'Main Content Menu Search Search Sign In Quick Links News Sports Radio Music Listen Live', '19', '19', 'Top Stories Local The National Opinion World Canada Politics Indigenous Business Health Entertainment Tech', 'News Investigates Go Public Shows About', 'News World', 'Sunday The British', '31', 'Social Sharing U.', '19', '1', '31', 'The Associated Press', '21', '2021', '9', '00', 'Last Updated', '21', 'Astra Zeneca', 'Westfield Stratford City', '18', 'Henry Nicholls', '31', '19', 'United States', 'The British', '31', '50', '15', '1', 'But U.', 'K. Health Secretary Matt Hancock', '120,000', '17.2', '8', '12', 'Prime Minister Boris Johnson', 'Theresa Tam', '19', '704', '19', 'Quebec City', 'The Marguerite', 'Quebec City', '283', '50', 'Michel Cloutier', '19', '22', '286', '39', 'South Africa', '$2,000', 'Starting Monday', '19', 'Navigating Canada', 'Navigating Canada', 'The National', '1', '8', '25', 'Rohan Jumani', 'Richard Vanderlubbe', '8', '25', 'Prime Minister Justin Trudeau', '48', 'Canada As', '11', '50', '845,065', '19', '31,385', '21,659', '1,087', '19', '13', '666', '15', 'In British Columbia', 'Abbotsford Teachers', 'Exclusive Frustration', '380', '19', '193', '95', '1', '8', '25', 'As Ontario', 'Peter Lin', '8', '25', 'New Brunswick', 'An Edmundston', 'Manoir Belle Vue', '19', 'Nova Scotia', '$1,000', '38', 'Eastern Health', '19', 'Mount Pearl Senior High School', '7', '19', '28', 'The Northwest Territories', '19', '111.2', '19', '62.7', 'Johns Hopkins University', '2.4', 'Anthony Fauci', '2022', '19', 'United States', '500,000', '19', 'The U.', '19', '11', '25', '497,823', 'Johns Hopkins University', 'In South Korea', '117,000', '27', 'South Korea', 'Lee Jin', 'The Associated Press', '10', '65', 'South Korea', 'Novavax Inc', '19', 'Tel Aviv', '18', 'Gil Cohen', 'Getty Images', 'In Israel', '19', '19', '49', 'The Canadian Press', 'Journalistic Standards', 'News Report Typo', 'Error Add', 'Comments To', 'Submission Guidelines', 'Footer Links My Account Profile', 'Gem Newsletters Connect', 'Facebook Twitter You', 'Tube Instagram Mobile', 'Podcasts Contact', 'Submit Feedback Help Centre Audience Relations', 'O. Box', '500', '5', '1', '6', '1', '866', '306', '4636', '1', '866', '220', '6045', 'Corporate Info Sitemap Reuse', 'Permission Terms', 'Use Privacy Jobs Our Unions Independent Producers Political Ads Registry Ad', 'Choices Services Ombudsman Public Appearances Commercial Services', 'Shop Doing Business', 'Us Renting Facilities Accessibility It', 'Closed Captioning', 'Described Video', 'Accessibility Accessibility Feedback', '2021', 'Visitez Radio']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "          \n",
    "          (?:[A-Z])(?:[a-z]+|\\.)(?:\\s+[A-Z](?:[a-z]+|\\.))*(?:\\s+[A-Z])(?:[a-z]+|\\.)\n",
    "                                         # proper names\n",
    "          | \\$\\d+\\s\\b[tr|b|m]illion\\b    # literal monetary amounts\n",
    "          | \\$?\\d+(?:[,\\.]\\d+)?          # numerical monetary amounts\n",
    "          | \\d{2}\\[\\\\]\\d{2}\\-\\\\]\\d{4}    # numerical dates (U.S. format)\n",
    "          | [A-Z][a-z.]*\\s\\d{2}\\,\\s\\d{2, 4} # literal dates (U.S. format)\n",
    "\n",
    "          \n",
    "        '''\n",
    "\n",
    "print(nltk.regexp_tokenize(nyt, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-buffalo",
   "metadata": {},
   "source": [
    "#####  10.\n",
    "\n",
    "☼ Rewrite the following loop as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "requested-birthday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "applicable-scoop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = [(word, len(word)) for word in sent]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-citation",
   "metadata": {},
   "source": [
    "##### 11.\n",
    "\n",
    "☼ Define a string `raw` containing a sentence of your own choosing. Now, split `raw` on some character other than space, such as '`s`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "potential-boards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How much ', ' would a ', 'chuck chuck if a ', 'chuck could chuck ', '?']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "raw.split('wood')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-smith",
   "metadata": {},
   "source": [
    "##### 12.\n",
    "\n",
    "☼ Write a `for` loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "compound-salem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "o\n",
      "m\n",
      "p\n",
      "a\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n"
     ]
    }
   ],
   "source": [
    "string = \"Compared to some of the previous exercises, this seems comically easy.\"\n",
    "\n",
    "for s in string[:20]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-heather",
   "metadata": {},
   "source": [
    "##### 13.\n",
    "\n",
    "☼ What is the difference between calling `split` on a string with no argument or with `' '` as the argument, e.g. `sent.split()` versus `sent.split(' ')`? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? \n",
    "\n",
    "*`sent.split()` splits all whitespace identically.*\n",
    "\n",
    "*`sent.split(' ')` splits all whitespace literally.  I.e., tabs will be represented as `\\t`, and each individaul whitespace will be spilt into its own string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dimensional-williams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'strings', 'has', 'tabs.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstrings\\thas\\ttabs.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'lots', 'of', 'space.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', '', '', '', '', '', '', '', 'string', '', '', '', '', '', '', '', '', '', 'has', '', '', '', '', '', 'lots', '', '', '', 'of', '', '', '', '', 'space.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'tabs', 'and', 'spaces.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstring', '', '', '', '', '', '', '', '', 'has\\ttabs', '', '', '', '', '', '', 'and\\tspaces.']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"This string is a pretty simple string.\"\n",
    "s2 = \"This\\tstrings\\thas\\ttabs.\"\n",
    "s3 = \"This        string          has      lots    of     space.\"\n",
    "s4 = \"This\\tstring         has\\ttabs       and\\tspaces.\"\n",
    "\n",
    "Ss = [s1, s2, s3, s4]\n",
    "\n",
    "for s in Ss:\n",
    "    print(\"\\nWith `sent.split()`:\")\n",
    "    print(s.split())\n",
    "    print(\"\\nWith `sent.split(' ')`:\")\n",
    "    print(s.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-queens",
   "metadata": {},
   "source": [
    "##### 14. \n",
    "\n",
    "☼ Create a variable `words` containing a list of words. Experiment with `words.sort()` and `sorted(words)`. What is the difference?\n",
    "\n",
    "*`words.sort()` doesn't return a value, but it alters the ordering of the list, so that whenever I call the list again, the returned list will be the ordered one, and not the one I originally stored.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "north-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "words = [\"slova\", \"ord\", \"Wörter\", \"λόγια\", \"words\", \"palabras\", \"sanat\", \n",
    "         \"mots\", \"focail\", \"szavak\", \"parole\", \"words\", \"woorden\", \"ord\", \n",
    "         \"słowa\", \"palavras\", \"từ ngữ\", \"ווערטער\"]\n",
    "\n",
    "words.sort()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "portable-meter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "infectious-there",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "words = [\"slova\", \"ord\", \"Wörter\", \"λόγια\", \"words\", \"palabras\", \"sanat\", \n",
    "         \"mots\", \"focail\", \"szavak\", \"parole\", \"words\", \"woorden\", \"ord\", \n",
    "         \"słowa\", \"palavras\", \"từ ngữ\", \"ווערטער\"]\n",
    "\n",
    "print(sorted(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "personalized-indiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slova', 'ord', 'Wörter', 'λόγια', 'words', 'palabras', 'sanat', 'mots', 'focail', 'szavak', 'parole', 'words', 'woorden', 'ord', 'słowa', 'palavras', 'từ ngữ', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-liverpool",
   "metadata": {},
   "source": [
    "##### 15. \n",
    "\n",
    "☼ Explore the difference between strings and integers by typing the following at a Python prompt: `\"3\" * 7` and `3 * 7`. Try converting between strings and integers using `int(\"3\")` and `str(3)`.\n",
    "\n",
    "*Multiplying a string $x$ by an integer $y$ will just cause $x$ to be printed to the console $y$ times:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "seven-jones",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "altered-egypt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aware-measure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"3\") * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "central-relative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(3) * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-chicken",
   "metadata": {},
   "source": [
    "##### 17. \n",
    "\n",
    "☼ What happens when the formatting strings `%6s` and `%-6s` are used to display strings that are longer than six characters?\n",
    "\n",
    "*This looks to be a legacy question from an older version of the book, since this is the older method of formatting in Python.  As the question is written, `%6s` won't do anything to a longer string:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "educational-coverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'another test'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"another test\"\n",
    "\n",
    "\"%6s\" % (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "chemical-audio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey   '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%-6s\" % (\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "faced-australia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anothe'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%.6s\" % (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-teaching",
   "metadata": {},
   "source": [
    "##### 18. \n",
    "\n",
    "◑ Read in some text from a corpus, tokenize it, and print the list of all *wh*-word types that occur. (*wh*-words in English are used in questions, relative clauses and exclamations: *who*, *which*, *what*, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?\n",
    "\n",
    "*This question is a little difficult to follow.  Most of the corpora we're using have texts that have already been tokenized, so the first part of this question seems a bit redundant.  However, just to play along,  I'll use the raw text version of one of the Project Gutenberg texts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "confirmed-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "raw = gutenberg.raw('bryant-stories.txt')\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "tokens = sorted(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "domestic-being",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whale', 'What', 'When', 'Whenever', 'Where', 'Whether', 'Whiff', 'While', 'Whirling', 'White', 'Who', 'Whose', 'Why', 'what', 'whatever', 'wheat', 'wheelbarrow', 'wheeled', 'when', 'whence', 'whenever', 'where', 'wherein', 'wherever', 'whether', 'which', 'while', 'whimpering', 'whin', 'whinny', 'whipped', 'whirlpool', 'whiruled', 'whisk', 'whisked', 'whisper', 'whisper_', 'whispered', 'whispering', 'whispers', 'whistle', 'whistled', 'white', 'white-haired', 'white-robed', 'whither', 'who', 'whole', 'wholly', 'whom', 'whose', 'why']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in tokens if re.search('^[Ww]h', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-science",
   "metadata": {},
   "source": [
    "##### 22. \n",
    "\n",
    "◑ Examine the results of processing the URL `http://news.bbc.co.uk/` using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly Javascript commands. You may also find that sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "numeric-synthesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronavirus: What\\'s happening in Canada and around the world Sunday | CBC News Skip to Main Content Menu Search Search Sign In Quick Links News Sports Radio Music Listen Live TV Watch COVID-19 Local updates Watch live COVID-19 tracker Vaccine tracker Top Stories Local The National Opinion World Canada Politics Indigenous Business Health Entertainment Tech & Science CBC News Investigates Go Public Shows About CBC News World · THE LATEST Coronavirus: What\\'s happening in Canada and around the world Sunday The British government declared Sunday that every adult in the country should get a first coronavirus vaccine shot by July 31, at least a month earlier than its previous target. Social Sharing U.K. aims to get all adults vaccinated against COVID-19 with 1st dose by July 31 The Associated Press · Posted: Feb 21, 2021 9:00 AM ET | Last Updated: February 21 A health worker prepares an injection with a dose of Astra Zeneca coronavirus vaccine at a vaccination centre at London\\'s Westfield Stratford City shopping centre in the U.K. on Feb. 18. (Henry Nicholls/Reuters) comments The latest: U.K. speeds up vaccinations, hoping all adults get first jab by July 31. Tam stresses need for Canadians to maintain precautions as variant cases mount. Canada\\'s new air travel rules go into effect Monday for testing, hotel quarantine. Almost half a million dead from COVID-19 in the United States. Israel starts reopening economy after nearly half its population receives vaccine. Coronavirus  tracker : The cases, hospitalizations and vaccinations in your area. Have a question about the coronavirus pandemic? Send your question to COVID@cbc.ca The British government declared Sunday that every adult in the country should get a first coronavirus vaccine shot by July 31, at least a month earlier than its previous target, as it prepared to set out a \"cautious\" plan to ease the U.K.\\'s lockdown. The new target also aims for everyone 50 and over and those with an underlying health condition to get t'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.cbc.ca/news/world/coronavirus-covid19-canada-world-february21-2021-1.5922099\"\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-aluminum",
   "metadata": {},
   "source": [
    "##### 24. \n",
    "\n",
    "◑ Try to write code to convert text into *hAck3r*, using regular expressions and substitution, where `e` → `3`, `i` → `1`, `o` → `0`, `l` → `|`, `s` → `5`, `.` → `5w33t!`, `ate` → `8`. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map `s` to two different values: `$` for word-initial `s`, and `5` for word-internal `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "spatial-whale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h3||0 5uck3r55w33t!  1 8 y0ur |unch5w33t!  1t wa5 d3|15h5w33t!'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Hello suckers.  I ate your lunch.  It was delish.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['ate', 'e', 'i', 'o', 'l', 's', '\\.']\n",
    "sub = ['8', '3', '1', '0', '|', '5', '5w33t!']\n",
    "\n",
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "prospective-fifty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%3+3r %1%3r %1ck3d a %3ck 0f %1ck|3d %3%%3r55w33t!'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Peter Piper picked a peck of pickled peppers.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['e', 'i', 'o', 'l', 's', 't', 'p', '\\.']\n",
    "sub = ['3', '1', '0', '|', '5', '+', '%', '5w33t!']\n",
    "\n",
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "statewide-syndicate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$u513 |1v35 1n m1551551pp15w33t!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Susie lives in Mississippi.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['ate', 'e', 'i', 'o', 'l', 't', r'\\bs', 's', '\\.']\n",
    "sub = ['8', '3', '1', '0', '|', '+', '$', '5', '5w33t!']\n",
    "\n",
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-butler",
   "metadata": {},
   "source": [
    "##### 30. \n",
    "\n",
    "◑ Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "sitting-design",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus: What's happening in Canada and around the world Sunday | CBC News Skip to Main Content Menu Search Search Sign In Quick Links News Sports Radio Music Listen Live TV Watch COVID-19 Local updates Watch live COVID-19 tracker Vaccine tracker\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cbc.ca/news/world/coronavirus-covid19-canada-world-february21-2021-1.5922099'\n",
    "\n",
    "to_be_stemmed = return_URL_contents(url)\n",
    "print(to_be_stemmed[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "false-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coronavirus', ':', 'What', \"'s\", 'happening', 'in', 'Canada', 'and', 'around', 'the', 'world', 'Sunday', '|', 'CBC', 'News', 'Skip', 'to', 'Main', 'Content', 'Menu', 'Search', 'Search', 'Sign', 'In', 'Quick', 'Links', 'News', 'Sports', 'Radio', 'Music', 'Listen', 'Live', 'TV', 'Watch', 'COVID-19', 'Local', 'updates', 'Watch', 'live', 'COVID-19', 'tracker', 'Vaccine', 'tracker', 'Top', 'Stories', 'Local', 'The', 'National', 'Opinion', 'World', 'Canada', 'Politics', 'Indigenous', 'Business', 'Health', 'Entertainment', 'Tech', '&', 'Science', 'CBC', 'News', 'Investigates', 'Go', 'Public', 'Shows', 'About', 'CBC', 'News', 'World', '·', 'THE', 'LATEST', 'Coronavirus', ':', 'What', \"'s\", 'happening', 'in', 'Canada', 'and', 'around', 'the', 'world', 'Sunday', 'The', 'British', 'government', 'declared', 'Sunday', 'that', 'every', 'adult', 'in', 'the', 'country', 'should', 'get', 'a', 'first', 'coronavirus']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(to_be_stemmed)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "successful-tamil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coronaviru', ':', 'what', \"'s\", 'happen', 'in', 'canada', 'and', 'around', 'the', 'world', 'sunday', '|', 'cbc', 'new', 'skip', 'to', 'main', 'content', 'menu', 'search', 'search', 'sign', 'In', 'quick', 'link', 'new', 'sport', 'radio', 'music', 'listen', 'live', 'TV', 'watch', 'covid-19', 'local', 'updat', 'watch', 'live', 'covid-19', 'tracker', 'vaccin', 'tracker', 'top', 'stori', 'local', 'the', 'nation', 'opinion', 'world', 'canada', 'polit', 'indigen', 'busi', 'health', 'entertain', 'tech', '&', 'scienc', 'cbc', 'new', 'investig', 'Go', 'public', 'show', 'about', 'cbc', 'new', 'world', '·', 'the', 'latest', 'coronaviru', ':', 'what', \"'s\", 'happen', 'in', 'canada', 'and', 'around', 'the', 'world', 'sunday', 'the', 'british', 'govern', 'declar', 'sunday', 'that', 'everi', 'adult', 'in', 'the', 'countri', 'should', 'get', 'a', 'first', 'coronaviru']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens[:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "liquid-answer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coronavir', ':', 'what', \"'s\", 'hap', 'in', 'canad', 'and', 'around', 'the', 'world', 'sunday', '|', 'cbc', 'new', 'skip', 'to', 'main', 'cont', 'menu', 'search', 'search', 'sign', 'in', 'quick', 'link', 'new', 'sport', 'radio', 'mus', 'list', 'liv', 'tv', 'watch', 'covid-19', 'loc', 'upd', 'watch', 'liv', 'covid-19', 'track', 'vaccin', 'track', 'top', 'story', 'loc', 'the', 'nat', 'opin', 'world', 'canad', 'polit', 'indig', 'busy', 'heal', 'entertain', 'tech', '&', 'sci', 'cbc', 'new', 'investig', 'go', 'publ', 'show', 'about', 'cbc', 'new', 'world', '·', 'the', 'latest', 'coronavir', ':', 'what', \"'s\", 'hap', 'in', 'canad', 'and', 'around', 'the', 'world', 'sunday', 'the', 'brit', 'govern', 'decl', 'sunday', 'that', 'every', 'adult', 'in', 'the', 'country', 'should', 'get', 'a', 'first', 'coronavir']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(t) for t in tokens[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-binary",
   "metadata": {},
   "source": [
    "##### 33.\n",
    "\n",
    "◑ The `index()` function can be used to look up items in sequences. For example, `'inexpressible'.index('e')` tells us the index of the first position of the letter `e`.\n",
    "\n",
    " * a. What happens when you look up a substring, e.g. `'inexpressible'.index('re')`?\n",
    " \n",
    " * b. Define a variable `words` containing a list of words. Now use `words.index()` to look up the position of an individual word.\n",
    " \n",
    " * c. Define a variable `silly` as in the exercise above. Use the `index()` function in combination with list slicing to build a list `phrase` consisting of all the words up to (but not including) `in` in `silly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "forward-shift",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'inexpressible'.index('re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "monthly-attraction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"I'm\", 'too', 'tired', 'think', 'of', 'a', 'more', \n",
    "         'original', 'list']\n",
    "\n",
    "words.index('tired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "annoying-thermal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "\n",
    "phrase = [i for i in silly.split()[:silly.split().index('in')]]\n",
    "\n",
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-cross",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-surveillance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
