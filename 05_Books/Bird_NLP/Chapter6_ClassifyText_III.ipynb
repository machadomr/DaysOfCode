{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adopted-being",
   "metadata": {},
   "source": [
    "# <center>Book: Steven Bird, Ewan Klein, Edward Loper, 2009. **Natural Language Processing (NLP) with Python**, O'Reilly.</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-tribe",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broke-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "celtic-processor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "affected-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endwsith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "floating-package",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6270512182993535"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words = brown.tagged_words(categories = 'news')\n",
    "featuresets = [(pos_features(n), g) for (n, g) in tagged_words]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "based-court",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNS'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(pos_features('cats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "basic-portland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endwsith(the) == False: \n",
      "  if endwsith(,) == False: \n",
      "    if endwsith(s) == False: \n",
      "      if endwsith(.) == False: \n",
      "        if endwsith(of) == False: \n",
      "          if endwsith(and) == False: return '.'\n",
      "          if endwsith(and) == True: return 'CC'\n",
      "        if endwsith(of) == True: return 'IN'\n",
      "      if endwsith(.) == True: return '.'\n",
      "    if endwsith(s) == True: \n",
      "      if endwsith(is) == False: \n",
      "        if endwsith(was) == False: \n",
      "          if endwsith(as) == False: return 'PP$'\n",
      "          if endwsith(as) == True: return 'CS'\n",
      "        if endwsith(was) == True: return 'BEDZ'\n",
      "      if endwsith(is) == True: \n",
      "        if endwsith(his) == False: return 'BEZ'\n",
      "        if endwsith(his) == True: return 'PP$'\n",
      "  if endwsith(,) == True: return ','\n",
      "if endwsith(the) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-network",
   "metadata": {},
   "source": [
    "### Exploiting Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aerial-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i - 1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "blond-calcium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev-word': 'an', 'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features(brown.sents()[0], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imperial-three",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7891596220785678"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = []\n",
    "tagged_sents = brown.tagged_sents(categories = 'news')\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append( (pos_features(untagged_sent, i), tag) )\n",
    "\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-party",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-worth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-guinea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-thomson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
